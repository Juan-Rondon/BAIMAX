#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¤– bAImax OLLAMA Integration - Chatbot Inteligente Local
========================================================

PROPÃ“SITO: Integra Ollama (Llama2 local) con el chatbot bAImax para 
proporcionar respuestas mÃ©dicas mÃ¡s naturales y precisas sin depender 
de APIs externas ni costos adicionales.

JUSTIFICACIÃ“N EN EL PROYECTO:
- Chatbot completamente gratuito y sin lÃ­mites de uso
- Respuestas mÃ©dicas contextualizadas y profesionales  
- Funciona sin conexiÃ³n a internet una vez configurado
- Privacidad total - datos mÃ©dicos no salen del equipo
- Diferenciador clave para SENASOFT 2025

ARQUITECTURA:
ğŸ—ï¸ DISEÃ‘O:
1. Cliente Ollama local ejecutando Llama2
2. API REST local en puerto 11434
3. IntegraciÃ³n con sistema bAImax existente
4. Fallback al chatbot bÃ¡sico si Ollama no disponible
5. Prompts especializados en contexto mÃ©dico colombiano

CASOS DE USO:
ğŸ‘©â€âš•ï¸ CONSULTAS MÃ‰DICAS: Respuestas naturales sobre sÃ­ntomas
ğŸš¨ EMERGENCIAS: OrientaciÃ³n inmediata y profesional  
ğŸ“Š ANÃLISIS: InterpretaciÃ³n de datos epidemiolÃ³gicos
ğŸ›ï¸ INSTITUCIONAL: ComunicaciÃ³n con ciudadanos

Desarrollado para IBM SENASOFT 2025 - InnovaciÃ³n en IA Local
"""

import requests
import json
import time
from typing import Dict, Any, Optional
import logging

class bAImaxOllama:
    """
    ğŸ¤– Motor de IA conversacional local para bAImax usando Ollama
    """
    
    def __init__(self, modelo: str = "llama2"):
        """
        Inicializa la conexiÃ³n con Ollama
        
        Args:
            modelo: Nombre del modelo a usar (llama2, mistral, etc.)
        """
        self.modelo = modelo
        self.url_base = "http://localhost:11434"
        self.disponible = False
        self.timeout = 30  # segundos
        
        # Configurar logging
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # Verificar disponibilidad
        self._verificar_conexion()
        
    def _verificar_conexion(self) -> bool:
        """
        Verifica si Ollama estÃ¡ ejecutÃ¡ndose y el modelo disponible
        
        Returns:
            bool: True si Ollama estÃ¡ disponible
        """
        try:
            # Verificar si Ollama estÃ¡ corriendo
            response = requests.get(f"{self.url_base}/api/tags", timeout=5)
            
            if response.status_code == 200:
                modelos = response.json()
                modelos_disponibles = [m['name'] for m in modelos.get('models', [])]
                
                if self.modelo in modelos_disponibles or any(self.modelo in m for m in modelos_disponibles):
                    self.disponible = True
                    self.logger.info(f"âœ… Ollama conectado - Modelo {self.modelo} disponible")
                    return True
                else:
                    self.logger.warning(f"âš ï¸ Modelo {self.modelo} no encontrado. Disponibles: {modelos_disponibles}")
                    return False
            else:
                self.logger.warning("âš ï¸ Ollama ejecutÃ¡ndose pero no responde correctamente")
                return False
                
        except requests.exceptions.RequestException as e:
            self.logger.warning(f"âš ï¸ Ollama no disponible: {e}")
            return False
    
    def _crear_prompt_medico(self, mensaje_usuario: str, contexto: Dict[str, Any] = None) -> str:
        """
        Crea un prompt especializado para consultas mÃ©dicas
        
        Args:
            mensaje_usuario: Mensaje del usuario
            contexto: InformaciÃ³n adicional (ciudad, edad, etc.)
            
        Returns:
            str: Prompt optimizado para respuestas mÃ©dicas
        """
        
        prompt_sistema = """Eres un asistente mÃ©dico inteligente llamado bAImax, especializado en el sistema de salud colombiano. 

INSTRUCCIONES IMPORTANTES:
- Proporciona informaciÃ³n mÃ©dica general y orientaciÃ³n
- NUNCA hagas diagnÃ³sticos definitivos
- Siempre recomienda consultar un profesional de salud
- Usa terminologÃ­a comprensible para ciudadanos colombianos
- Si es una emergencia, recomienda llamar al 123 o acudir a urgencias
- SÃ© empÃ¡tico y profesional
- Respuestas concisas de mÃ¡ximo 150 palabras
- Incluye emojis mÃ©dicos apropiados

CONTEXTO DEL SISTEMA:
- Eres parte de bAImax 2.0, sistema de IBM SENASOFT 2025
- Ayudas a clasificar gravedad de casos mÃ©dicos
- Trabajas con datos del Ministerio de Salud de Colombia"""

        if contexto:
            info_contexto = f"\nCONTEXTO ADICIONAL:\n"
            if 'ciudad' in contexto:
                info_contexto += f"- Ciudad: {contexto['ciudad']}\n"
            if 'edad' in contexto:
                info_contexto += f"- Edad: {contexto['edad']} aÃ±os\n"
            if 'gravedad_detectada' in contexto:
                info_contexto += f"- Gravedad detectada por IA: {contexto['gravedad_detectada']}\n"
        else:
            info_contexto = ""
            
        prompt_completo = f"{prompt_sistema}{info_contexto}\n\nCONSULTA DEL USUARIO: {mensaje_usuario}\n\nRESPUESTA MÃ‰DICA:"
        
        return prompt_completo
    
    def generar_respuesta(self, mensaje: str, contexto: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Genera una respuesta usando Ollama
        
        Args:
            mensaje: Mensaje del usuario
            contexto: Contexto adicional opcional
            
        Returns:
            Dict con la respuesta y metadatos
        """
        
        if not self.disponible:
            return {
                'respuesta': "ğŸ¤– El asistente IA avanzado no estÃ¡ disponible. Usando modo bÃ¡sico.",
                'exito': False,
                'tiempo_ms': 0,
                'modelo_usado': 'basic'
            }
        
        try:
            inicio = time.time()
            
            # Crear prompt especializado
            prompt = self._crear_prompt_medico(mensaje, contexto)
            
            # Configurar request para Ollama
            payload = {
                "model": self.modelo,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 200,
                    "stop": ["\n\nCONSULTA DEL USUARIO:", "RESPUESTA MÃ‰DICA:"]
                }
            }
            
            # Hacer request a Ollama
            response = requests.post(
                f"{self.url_base}/api/generate",
                json=payload,
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                resultado = response.json()
                respuesta_texto = resultado.get('response', '').strip()
                
                # Limpiar respuesta
                respuesta_limpia = self._limpiar_respuesta(respuesta_texto)
                
                tiempo_total = int((time.time() - inicio) * 1000)
                
                self.logger.info(f"âœ… Respuesta generada en {tiempo_total}ms")
                
                return {
                    'respuesta': respuesta_limpia,
                    'exito': True,
                    'tiempo_ms': tiempo_total,
                    'modelo_usado': self.modelo,
                    'tokens_usados': len(respuesta_texto.split())
                }
            else:
                self.logger.error(f"âŒ Error de Ollama: {response.status_code}")
                return {
                    'respuesta': "ğŸ¤– Error temporal del asistente IA. Intente nuevamente.",
                    'exito': False,
                    'tiempo_ms': int((time.time() - inicio) * 1000),
                    'modelo_usado': 'error'
                }
                
        except requests.exceptions.Timeout:
            self.logger.error("âŒ Timeout en respuesta de Ollama")
            return {
                'respuesta': "ğŸ¤– El asistente estÃ¡ procesando. Intente con una consulta mÃ¡s especÃ­fica.",
                'exito': False,
                'tiempo_ms': self.timeout * 1000,
                'modelo_usado': 'timeout'
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Error inesperado: {e}")
            return {
                'respuesta': "ğŸ¤– Error del sistema. Por favor, contacte soporte tÃ©cnico.",
                'exito': False,
                'tiempo_ms': 0,
                'modelo_usado': 'error'
            }
    
    def _limpiar_respuesta(self, respuesta: str) -> str:
        """
        Limpia y formatea la respuesta de Ollama
        
        Args:
            respuesta: Texto crudo de Ollama
            
        Returns:
            str: Respuesta limpia y formateada
        """
        
        # Eliminar texto del sistema que pueda haberse colado
        limpiadores = [
            "RESPUESTA MÃ‰DICA:",
            "CONSULTA DEL USUARIO:",
            "INSTRUCCIONES IMPORTANTES:",
            "CONTEXTO DEL SISTEMA:"
        ]
        
        respuesta_limpia = respuesta
        for limpiador in limpiadores:
            respuesta_limpia = respuesta_limpia.replace(limpiador, "").strip()
        
        # Asegurar que empiece con emoji si no lo tiene
        if respuesta_limpia and not any(emoji in respuesta_limpia[:5] for emoji in ['ğŸ¥', 'ğŸ¤–', 'âš ï¸', 'ğŸš¨', 'ğŸ’Š', 'ğŸ”¬']):
            respuesta_limpia = f"ğŸ¥ {respuesta_limpia}"
        
        # Limitar longitud
        if len(respuesta_limpia) > 400:
            respuesta_limpia = respuesta_limpia[:397] + "..."
        
        return respuesta_limpia
    
    def verificar_modelo_disponible(self) -> bool:
        """
        Reververifica si el modelo estÃ¡ disponible
        
        Returns:
            bool: Estado actual de disponibilidad
        """
        return self._verificar_conexion()
    
    def obtener_info_sistema(self) -> Dict[str, Any]:
        """
        Obtiene informaciÃ³n del sistema Ollama
        
        Returns:
            Dict con informaciÃ³n del sistema
        """
        
        if not self.disponible:
            return {
                'estado': 'No disponible',
                'modelo': self.modelo,
                'url': self.url_base
            }
        
        try:
            response = requests.get(f"{self.url_base}/api/tags", timeout=5)
            if response.status_code == 200:
                info = response.json()
                return {
                    'estado': 'Conectado',
                    'modelo': self.modelo,
                    'url': self.url_base,
                    'modelos_disponibles': [m['name'] for m in info.get('models', [])],
                    'version_ollama': 'EjecutÃ¡ndose'
                }
        except Exception as e:
            return {
                'estado': f'Error: {e}',
                'modelo': self.modelo,
                'url': self.url_base
            }

# =============================================================================
# FUNCIONES DE UTILIDAD PARA INTEGRACIÃ“N
# =============================================================================

def probar_ollama() -> None:
    """
    FunciÃ³n de prueba para verificar Ollama
    """
    print("ğŸ¤– Probando conexiÃ³n con Ollama...")
    
    ollama = bAImaxOllama()
    info = ollama.obtener_info_sistema()
    
    print(f"ğŸ“Š Estado: {info['estado']}")
    if 'modelos_disponibles' in info:
        print(f"ğŸ“š Modelos: {', '.join(info['modelos_disponibles'])}")
    
    if ollama.disponible:
        print("\nğŸ§ª Probando respuesta...")
        respuesta = ollama.generar_respuesta(
            "Hola, tengo dolor de cabeza leve",
            contexto={'ciudad': 'BogotÃ¡', 'edad': 30}
        )
        
        print(f"âœ… Respuesta ({respuesta['tiempo_ms']}ms): {respuesta['respuesta']}")
    else:
        print("âŒ Ollama no disponible. Verifique instalaciÃ³n.")

if __name__ == "__main__":
    probar_ollama()